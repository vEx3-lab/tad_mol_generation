import pandas as pd
import pickle
from tqdm import tqdm
from dataloader import SMILESTokenizer, Vocabulary, create_vocabulary, SmilesDataset
# -----------------------------
# å‚æ•°é…ç½®
# -----------------------------
INPUT_CSV = "../data/htvs_molecules_with_selfies.csv"       # è¾“å…¥æ–‡ä»¶
OUTPUT_PICKLE = "../data/preprocessed.pkl"  # é¢„å¤„ç†è¾“å‡ºæ–‡ä»¶
MAX_LEN = 120                       # æœ€å¤§é•¿åº¦ï¼ˆæ ¹æ®æ•°æ®å¯è°ƒæ•´ï¼‰

# -----------------------------
# 1. è¯»å–åŸå§‹æ•°æ®
# -----------------------------
print("ğŸ“¥ Loading SMILES data ...")
df = pd.read_csv(INPUT_CSV)
smiles_list = df["smiles"].astype(str).tolist()
print(f"Loaded {len(smiles_list)} SMILES samples")

# -----------------------------
# 2. æ„å»ºåˆ†è¯å™¨å’Œè¯è¡¨
# -----------------------------
print("ğŸ”¤ Building tokenizer & vocabulary ...")
tokenizer = SMILESTokenizer()
vocab = create_vocabulary(smiles_list, tokenizer)

print(f"Vocab size = {len(vocab)}")
print("Example tokens:", list(vocab.tokens())[:20])

# -----------------------------
# 3. å°† SMILES è½¬æ¢ä¸º token id åºåˆ—
# -----------------------------
print("ğŸ§© Tokenizing and encoding SMILES ...")
encoded_data = []
for smi in tqdm(smiles_list):
    tokens = tokenizer.tokenize(smi)
    ids = vocab.encode(tokens)
    if len(ids) < MAX_LEN:
        ids += [vocab["<pad>"]] * (MAX_LEN - len(ids))
    else:
        ids = ids[:MAX_LEN]
    encoded_data.append(ids)

# -----------------------------
# 4. ä¿å­˜é¢„å¤„ç†ç»“æœ
# -----------------------------
data_dict = {
    "smiles": smiles_list,
    "encoded": encoded_data,
    "vocab_tokens": vocab.tokens(),
    "vocab_dict": vocab._tokens
}

with open(OUTPUT_PICKLE, "wb") as f:
    pickle.dump(data_dict, f)

print(f"âœ… Done! Preprocessed data saved to {OUTPUT_PICKLE}")
